# -*- coding: utf-8 -*-
"""NN_HW7_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14CrlSpsUlvlWb5eXQjVx3YpjaRUb-Ipq
"""

import keras
from matplotlib import pyplot as plt
from IPython.display import clear_output
import unicodedata
import re
import numpy as np
from numpy import array
from keras.utils import to_categorical
import tensorflow as tf
from keras.utils.vis_utils import plot_model
from keras.models import Sequential
from keras.layers import LSTM, GRU, Bidirectional
from keras.layers import Dense
from keras.layers import Embedding
from keras.layers import RepeatVector
from keras.layers import TimeDistributed
from keras.callbacks import ModelCheckpoint
import pickle
from keras.utils import to_categorical


class PlotLosses(keras.callbacks.Callback):
  
    def on_train_begin(self, logs={}):
        self.i = 0
        self.x = []
        self.losses = []
        self.val_losses = []        
        self.fig = plt.figure()
        self.logs = []

    def on_epoch_end(self, epoch, logs={}):
        
        self.logs.append(logs)
        self.x.append(self.i)
        self.losses.append(logs.get('loss'))
        self.val_losses.append(logs.get('val_loss'))
        self.i += 1
        
        clear_output(wait=True)
        plt.plot(self.x, self.losses, label="loss")
        plt.plot(self.x, self.val_losses, label="val_loss")
        plt.legend()
        plt.show();
        
plot_losses = PlotLosses()




def unicode_to_ascii(s):
    return ''.join(c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn')

  
def encode_output(sequences, vocab_size):
    ylist = list()
    for sequence in sequences:
        encoded = to_categorical(sequence, num_classes=vocab_size)
        ylist.append(encoded)
    y = array(ylist)
    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)
    return y

def preprocess_sentence(w):
  
    w = unicode_to_ascii(w.lower().strip())
    w = re.sub(r"([?.!,¿])", r" \1 ", w)
    w = re.sub(r'[" "]+', " ", w)
    
    w = re.sub(r"[^a-zA-Z?.!,¿]+", " ", w)
    
    w = w.rstrip().strip()
       
    w = '<start> ' + w + ' <end>'
    
    return w
  
def preprocess_persian_sentence(row):
    res = row.split('\t')
    eng =(res[0])
    per = (res[1])
    eng = unicode_to_ascii(eng.lower().strip())
    per = unicode_to_ascii(per.lower().strip())
    # per = re.sub('(.*?)', ' ', per)
    if(len(per.split('.'))>1):
        per = per.split('.')[0] + ' . '

    if (len(eng.split('.')) > 1):
        eng = eng.split('.')[0] + ' . '

    if (len(per.split('!')) > 1):
        per = per.split('!')[0] + ' ! '

    # eng = unicode_to_ascii(w.lower().strip())
    eng = re.sub(r"([?.!,¿؟])", r" \1 ", eng)
    per = re.sub(r"([?.!,¿،؟])", r" \1 ", per)
    eng = re.sub(r'[" "]+', " ", eng)
    per = re.sub(r'(" ")+', " ", per)
    eng = re.sub(r"[^a-zA-Z?.!,¿،؟]+", " ", eng)

    eng = eng.rstrip().strip()
    per = '<start> ' + per + ' <end>'
    eng = '<start> ' + eng + ' <end>'
    return [eng, per]
  
def create_dataset(path):
    
    num_examples = 21000
      
    lines = open(path, encoding='UTF-8').read().strip().split('\n')
   
    
    word_pairs = [[preprocess_sentence(w) for w in l.split('\t')]  for l in lines[:num_examples]]
        
#     word_pairs = [ preprocess_persian_sentence(l) for l in lines[:num_examples]]
    
    
    shuffle(word_pairs)

    import pickle

    with open('data.pkl', 'wb') as input:
      pickle.dump(word_pairs, input)
    
    return 
  
  
class LanguageIndex():
  def __init__(self, lang):
    self.lang = lang
    self.word2idx = {}
    self.idx2word = {}
    self.vocab = set()
    
    self.create_index()
    
  def create_index(self):
    for phrase in self.lang:
      self.vocab.update(phrase.split(' '))
    
    self.vocab = sorted(self.vocab)
    
    self.word2idx['<pad>'] = 0
    for index, word in enumerate(self.vocab):
      self.word2idx[word] = index + 1
    
    for word, index in self.word2idx.items():
      self.idx2word[index] = word
      
def max_length(tensor):
    
    return max(len(t) for t in tensor)



  
def encode_output(sequences, vocab_size):
    ylist = list()
    for sequence in sequences:
        encoded = to_categorical(sequence, num_classes=vocab_size)
        ylist.append(encoded)
    y = array(ylist)
    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)
    return y

from numpy.random import shuffle 
  
def load_dataset():
  
    
    with open('data.pkl', 'rb') as input:
      pairs = pickle.load( input)
    
    inp_lang = LanguageIndex(en for en, sp in pairs)
    targ_lang = LanguageIndex(sp for en, sp in pairs)
    
    vocab_inp_size = len(inp_lang.word2idx)
    vocab_tar_size = len(targ_lang.word2idx)
    
    input_tensor = [[inp_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]
    
    target_tensor = [[targ_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]

    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)
    
    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, 
                                                                 maxlen=max_length_inp,
                                                                 padding='post')
    
    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, 
                                                                  maxlen=max_length_tar, 
                                                                  padding='post')

    target_tensor = encode_output(target_tensor, vocab_tar_size) 
    
    input_tensor_train = input_tensor[0:20000]
    input_tensor_val = input_tensor[20000:21000]
    
    target_tensor_train = target_tensor[0:20000]
    target_tensor_val = target_tensor[20000:21000]


    target_input_sent = pairs[0:20000][1]
    target_val_sent = pairs[20000:21000][1]
    
    return input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val, inp_lang, targ_lang, max_length_inp, max_length_tar, pairs

create_dataset('spa.txt')

input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val, inp_lang, targ_lang, max_length_inp, max_length_targ, pairs = load_dataset()

embedding_dim = 256
units = 1024
vocab_inp_size = len(inp_lang.word2idx)
vocab_tar_size = len(targ_lang.word2idx)

print(max_length_targ)
print(vocab_inp_size)
print(vocab_tar_size)
print(max_length_inp)
print(max_length_targ)





def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):
    model = Sequential()
    
#     Encoder
    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))
#     model.add(LSTM(n_units, return_sequences=True))
#     model.add(LSTM(n_units, return_sequences=True))
#     model.add(LSTM(n_units, return_sequences=True))    
    model.add(LSTM(n_units))
    model.add(RepeatVector(tar_timesteps))
    
#     decoder
    model.add(LSTM(n_units, return_sequences=True))
#     model.add(LSTM(n_units, return_sequences=True))
#     model.add(LSTM(n_units, return_sequences=True))
#     model.add(LSTM(n_units, return_sequences=True))    
    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))
    
    return model

model = define_model(10, 10, 10, 10, 10)
model.compile(optimizer='rmsProp', loss='categorical_crossentropy')



filename = 'LSTM'

print(model.summary())

plot_model(model, to_file= filename + '.png', show_shapes=True)

checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
model.fit(input_tensor_train, target_tensor_train, epochs=100, batch_size=100,validation_data=(input_tensor_val, target_tensor_val), callbacks=[plot_losses, checkpoint], verbose=2)

model.save(filename)

from nltk.translate.bleu_score import corpus_bleu

def predict_sent(input_sent):
  
  predictions = model.predict(input_sent, verbose=0)[0]

  integers = [np.argmax(vector) for vector in predictions]
	
  target = list()
	
  for i in integers:
    word = targ_lang.idx2word[i]
    if i == 0:
      break
    target.append(word)
    
  return (target)
           
def evaluate_model(input_sentences, raw_dataset):
  
  actual = []
  pred = []
  
  for i in range(0, len(input_sentences)):
    
    translation = predict_sent(input_sentences[i:i+1])
    _, raw_target = raw_dataset[i]

    actual.append(raw_target.split())
    pred.append(translation)
  
  print(actual)
  print(pred)
  
  print('BLEU-1: %f' % corpus_bleu(pred, actual, weights=(1.0, 0, 0, 0)))
  print('BLEU-2: %f' % corpus_bleu(pred, actual, weights=(0.5, 0.5, 0, 0)))
  print('BLEU-3: %f' % corpus_bleu(pred, actual, weights=(0.3, 0.3, 0.3, 0)))
  print('BLEU-4: %f' % corpus_bleu(pred, actual, weights=(0.25, 0.25, 0.25, 0.25)))

evaluate_model(input_tensor_train, pairs)

evaluate_model(input_tensor_val,pairs[1800:2000])


